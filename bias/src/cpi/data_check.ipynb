{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundProteinInteractionPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompoundProteinInteractionPrediction, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        self.embed_word = nn.Embedding(n_word, dim)\n",
    "        self.W_gnn = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer_gnn)])\n",
    "        self.W_cnn = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2*window+1,stride=1, padding=window) for _ in range(layer_cnn)])\n",
    "        self.W_attention = nn.Linear(dim, dim)\n",
    "        self.W_out = nn.ModuleList([nn.Linear(2*dim, 2*dim) for _ in range(layer_output)])\n",
    "        self.W_interaction = nn.Linear(2*dim, 2)\n",
    "\n",
    "    def gnn(self, xs, A, layer):\n",
    "        for i in range(layer):\n",
    "            hs = torch.relu(self.W_gnn[i](xs))\n",
    "            xs = xs + torch.matmul(A, hs)\n",
    "        # return torch.unsqueeze(torch.sum(xs, 0), 0)\n",
    "        return torch.unsqueeze(torch.mean(xs, 0), 0)\n",
    "\n",
    "    def attention_cnn(self, x, xs, layer):\n",
    "        \"\"\"The attention mechanism is applied to the last layer of CNN.\"\"\"\n",
    "\n",
    "        xs = torch.unsqueeze(torch.unsqueeze(xs, 0), 0)\n",
    "        for i in range(layer):\n",
    "            xs = torch.relu(self.W_cnn[i](xs))\n",
    "        xs = torch.squeeze(torch.squeeze(xs, 0), 0)\n",
    "\n",
    "        h = torch.relu(self.W_attention(x))\n",
    "        hs = torch.relu(self.W_attention(xs))\n",
    "        weights = torch.tanh(F.linear(h, hs))\n",
    "        ys = torch.t(weights) * hs\n",
    "\n",
    "        # return torch.unsqueeze(torch.sum(ys, 0), 0)\n",
    "        return torch.unsqueeze(torch.mean(ys, 0), 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fingerprints, adjacency, words = inputs\n",
    "\n",
    "        \"\"\"Compound vector with GNN.\"\"\"\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "        compound_vector = self.gnn(fingerprint_vectors, adjacency, layer_gnn)\n",
    "\n",
    "        \"\"\"Protein vector with attention-CNN.\"\"\"\n",
    "        word_vectors = self.embed_word(words)\n",
    "        protein_vector = self.attention_cnn(compound_vector, word_vectors, layer_cnn)\n",
    "\n",
    "        \"\"\"Concatenate the above two vectors and output the interaction.\"\"\"\n",
    "        cat_vector = torch.cat((compound_vector, protein_vector), 1)\n",
    "        for j in range(layer_output):\n",
    "            cat_vector = torch.relu(self.W_out[j](cat_vector))\n",
    "        interaction = self.W_interaction(cat_vector)\n",
    "\n",
    "        return interaction\n",
    "\n",
    "    def __call__(self, data, train=True):\n",
    "\n",
    "        inputs, correct_interaction = data[:-1], data[-1]\n",
    "        predicted_interaction = self.forward(inputs)\n",
    "\n",
    "        if train:\n",
    "            loss = F.cross_entropy(predicted_interaction, correct_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            correct_labels = correct_interaction.to('cpu').data.numpy()\n",
    "            ys = F.softmax(predicted_interaction, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            return correct_labels, predicted_labels, predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for data in dataset:\n",
    "            loss = self.model(data)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset):\n",
    "        N = len(dataset)\n",
    "        T, Y, S = [], [], []\n",
    "        for data in dataset:\n",
    "            (correct_labels, predicted_labels,predicted_scores) = self.model(data, train=False)\n",
    "            T.append(correct_labels)\n",
    "            Y.append(predicted_labels)\n",
    "            S.append(predicted_scores)\n",
    "        AUC = roc_auc_score(T, S)\n",
    "        precision = precision_score(T, Y)\n",
    "        recall = recall_score(T, Y)\n",
    "        return AUC, precision, recall\n",
    "\n",
    "    def save_AUCs(self, AUCs, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write('\\t'.join(map(str, AUCs)) + '\\n')\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters.\"\"\"\n",
    "DATASET=\"human\"\n",
    "# DATASET=celegans\n",
    "# DATASET=yourdata\n",
    "\n",
    "# radius=1\n",
    "radius=\"2\"\n",
    "# radius=3\n",
    "\n",
    "# ngram=2\n",
    "ngram=\"3\"\n",
    "\n",
    "dim=10\n",
    "layer_gnn=3\n",
    "side=5\n",
    "window=((2*side+1))\n",
    "layer_cnn=3\n",
    "layer_output=3\n",
    "lr=1e-3\n",
    "lr_decay=0.5\n",
    "decay_interval=10\n",
    "weight_decay=1e-6\n",
    "iteration=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"Hyperparameters.\"\"\"\n",
    "    # (DATASET, radius, ngram, dim, layer_gnn, window, layer_cnn, layer_output,\n",
    "    #  lr, lr_decay, decay_interval, weight_decay, iteration,\n",
    "    #  setting) = sys.argv[1:]\n",
    "    # (dim, layer_gnn, window, layer_cnn, layer_output, decay_interval,\n",
    "    #  iteration) = map(int, [dim, layer_gnn, window, layer_cnn, layer_output,\n",
    "    #                         decay_interval, iteration])\n",
    "    # lr, lr_decay, weight_decay = map(float, [lr, lr_decay, weight_decay])\n",
    "\n",
    "    \"\"\"CPU or GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "        print('The code uses GPU...')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU!!!')\n",
    "\n",
    "    \"\"\"Load preprocessed data.\"\"\"\n",
    "    dir_input = ('./data/' + DATASET + '/input/'\n",
    "                 'radius' + radius + '_ngram' + ngram + '/')\n",
    "    compounds = load_tensor(dir_input + 'compounds', torch.LongTensor)\n",
    "    adjacencies = load_tensor(dir_input + 'adjacencies', torch.FloatTensor)\n",
    "    proteins = load_tensor(dir_input + 'proteins', torch.LongTensor)\n",
    "    interactions = load_tensor(dir_input + 'interactions', torch.LongTensor)\n",
    "    fingerprint_dict = load_pickle(dir_input + 'fingerprint_dict.pickle')\n",
    "    word_dict = load_pickle(dir_input + 'word_dict.pickle')\n",
    "    n_fingerprint = len(fingerprint_dict)\n",
    "    n_word = len(word_dict)\n",
    "\n",
    "    \"\"\"Create a dataset and split it into train/dev/test.\"\"\"\n",
    "    dataset = list(zip(compounds, adjacencies, proteins, interactions))\n",
    "    dataset = shuffle_dataset(dataset, 1234)\n",
    "    dataset_train, dataset_ = split_dataset(dataset, 0.8)\n",
    "    dataset_dev, dataset_test = split_dataset(dataset_, 0.5)\n",
    "    print(len(dataset))\n",
    "\n",
    "    \"\"\"Set a model.\"\"\"\n",
    "    torch.manual_seed(1234)\n",
    "    model = CompoundProteinInteractionPrediction().to(device)\n",
    "    trainer = Trainer(model)\n",
    "    tester = Tester(model)\n",
    "\n",
    "    \"\"\"Output files.\"\"\"\n",
    "    file_AUCs = '../../output/cpi/result/AUCs_new.txt'\n",
    "    file_model = '../../output/cpi/model/model_new'\n",
    "    AUCs = ('Epoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\t'\n",
    "            'AUC_test\\tPrecision_test\\tRecall_test')\n",
    "    with open(file_AUCs, 'w') as f:\n",
    "        f.write(AUCs + '\\n')\n",
    "\n",
    "    \"\"\"Start training.\"\"\"\n",
    "    print('Training...')\n",
    "    print(AUCs)\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(1, iteration):\n",
    "\n",
    "        if epoch % decay_interval == 0:\n",
    "            trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "        loss_train = trainer.train(dataset_train)\n",
    "        AUC_dev = tester.test(dataset_dev)[0]\n",
    "        AUC_test, precision_test, recall_test = tester.test(dataset_test)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        time = end - start\n",
    "\n",
    "        AUCs = [epoch, time, loss_train, AUC_dev,\n",
    "                AUC_test, precision_test, recall_test]\n",
    "        tester.save_AUCs(AUCs, file_AUCs)\n",
    "        tester.save_model(model, file_model)\n",
    "\n",
    "        print('\\t'.join(map(str, AUCs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d396b96c37022cc03b590fb7e0f59e9625b1a542a0cf65b03ce3992dff67545"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('compound': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
