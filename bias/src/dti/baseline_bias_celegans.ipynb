{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rdic = ['A','I','L','V','F','W','Y','N','C','Q','M','S','T','D','E','R','H','K','G','P','O','U','X','B','Z']\n",
    "seq_dic = {w: i+1 for i, w in enumerate(seq_rdic)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeSeq(seq, seq_dic):\n",
    "    if pd.isnull(seq):\n",
    "        return [0]\n",
    "    else:\n",
    "        return [seq_dic[aa] for aa in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(x, max_len=2500):\n",
    "    if len(x) == 0:\n",
    "        return x\n",
    "    elif len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [0]*(max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(dti_dir, prot_len=2500, drug_len=2048):\n",
    "\n",
    "    protein_col = \"protein\"\n",
    "    drug_col = \"compound\"\n",
    "    label_col = \"label\"\n",
    "    col_names = [protein_col, drug_col, label_col]\n",
    "    \n",
    "    dti_df = pd.read_csv(dti_dir, header=0)\n",
    "\n",
    "    dti_df[protein_col] = dti_df[protein_col].map(lambda a: encodeSeq(a, seq_dic))\n",
    "    \n",
    "    drug_feature = np.stack(dti_df[drug_col].map(\n",
    "        lambda sm: AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(sm),2,nBits=drug_len)\n",
    "    ))\n",
    "    \n",
    "    protein_feature = np.stack(dti_df[protein_col].map(padding_seq))\n",
    "    label = dti_df[label_col].values\n",
    "    \n",
    "    print(\"\\tPositive data : %d\" %(sum(dti_df[label_col])))\n",
    "    print(\"\\tNegative data : %d\" %(dti_df.shape[0] - sum(dti_df[label_col])))\n",
    "    \n",
    "    return {\"protein_feature\": protein_feature, \n",
    "            \"drug_feature\": drug_feature, \n",
    "            \"label\": label,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:59:19] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:59:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPositive data : 2000\n",
      "\tNegative data : 1949\n",
      "\tPositive data : 401\n",
      "\tNegative data : 377\n"
     ]
    }
   ],
   "source": [
    "train_datas = parse_data(\"../../data/maked/bias/train_celegans.csv\")\n",
    "test_datas = parse_data(\"../../data/maked/bias/test_celegans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_drug = nn.Linear(2048, 64)\n",
    "        \n",
    "        self.conv1D = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=128, stride=1, padding=0)\n",
    "        self.batch = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc_out = nn.Linear(128, 128)\n",
    "        self.fc_interaction = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        compound_vector = self.fc_drug(drug)\n",
    "        compound_vector = torch.relu(compound_vector)\n",
    "\n",
    "        protein = torch.unsqueeze(protein, 1)\n",
    "        protein_vector = self.conv1D(protein)\n",
    "        protein_vector = self.batch(protein_vector)\n",
    "        protein_vector = torch.relu(protein_vector)\n",
    "        \n",
    "        protein_vector = F.max_pool1d(protein_vector, kernel_size=2373)\n",
    "        protein_vector = torch.squeeze(protein_vector, 2)\n",
    "        \n",
    "        cat_vector = torch.cat((compound_vector, protein_vector), 1)\n",
    "        \n",
    "        for j in range(2):\n",
    "            cat_vector = torch.relu(self.fc_out(cat_vector))\n",
    "        out = self.fc_interaction(cat_vector)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses GPU...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('The code uses GPU...')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('The code uses CPU!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drug_dataset = torch.FloatTensor(train_datas[\"drug_feature\"])\n",
    "train_protein_dataset = torch.FloatTensor(train_datas[\"protein_feature\"])\n",
    "train_target_dataset = torch.LongTensor(train_datas[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drug_dataset = torch.FloatTensor(test_datas[\"drug_feature\"])\n",
    "test_protein_dataset = torch.FloatTensor(test_datas[\"protein_feature\"])\n",
    "test_target_dataset = torch.LongTensor(test_datas[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_drug_dataset, train_protein_dataset, train_target_dataset)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_drug_dataset, test_protein_dataset, test_target_dataset)\n",
    "\n",
    "N = len(train_dataset)\n",
    "train_size = int(0.8 * N)\n",
    "val_size = N - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 68.42987483739853, AUC: 0.6209940006153215\n",
      "epoch: 1, loss: 67.82765030860901, AUC: 0.6486578299661574\n",
      "epoch: 2, loss: 67.19963890314102, AUC: 0.6738988308891396\n",
      "epoch: 3, loss: 66.1663846373558, AUC: 0.7000307660752744\n",
      "epoch: 4, loss: 65.11267650127411, AUC: 0.7209837452568968\n",
      "epoch: 5, loss: 63.41013306379318, AUC: 0.7492308481181418\n",
      "epoch: 6, loss: 62.12422722578049, AUC: 0.7645818377602298\n",
      "epoch: 7, loss: 60.42972034215927, AUC: 0.779368782688955\n",
      "epoch: 8, loss: 59.141819536685944, AUC: 0.7869513383242744\n",
      "epoch: 9, loss: 56.31357243657112, AUC: 0.8036419341605989\n",
      "epoch: 10, loss: 55.15802103281021, AUC: 0.8104617475130755\n",
      "epoch: 11, loss: 51.932824552059174, AUC: 0.8219349297507947\n",
      "epoch: 12, loss: 49.342913031578064, AUC: 0.8301135780945543\n",
      "epoch: 13, loss: 47.68669009208679, AUC: 0.8383627320274843\n",
      "epoch: 14, loss: 45.4947569668293, AUC: 0.8465862475643523\n",
      "epoch: 15, loss: 44.509271889925, AUC: 0.852008768331453\n",
      "epoch: 16, loss: 42.75587993860245, AUC: 0.8556045533791407\n",
      "epoch: 17, loss: 41.97517254948616, AUC: 0.8566557276176803\n",
      "epoch: 18, loss: 41.12799945473671, AUC: 0.8597964311352682\n",
      "epoch: 19, loss: 40.460500091314316, AUC: 0.8626038355040508\n",
      "epoch: 20, loss: 40.39457017183304, AUC: 0.8648023279663624\n",
      "epoch: 21, loss: 38.96547904610634, AUC: 0.8666579068813456\n",
      "epoch: 22, loss: 38.84098955988884, AUC: 0.8659079837965336\n",
      "epoch: 23, loss: 38.930790424346924, AUC: 0.8686352681776227\n",
      "epoch: 24, loss: 38.41353511810303, AUC: 0.8694524920520973\n",
      "epoch: 25, loss: 38.28113040328026, AUC: 0.8713817813557583\n",
      "epoch: 26, loss: 38.024778455495834, AUC: 0.8719618500666599\n",
      "epoch: 27, loss: 37.73532655835152, AUC: 0.8701158855501998\n",
      "epoch: 28, loss: 38.09691882133484, AUC: 0.8769837708952928\n",
      "epoch: 29, loss: 37.62052321434021, AUC: 0.8748910368167366\n",
      "epoch: 30, loss: 37.46943202614784, AUC: 0.8821178597066968\n",
      "epoch: 31, loss: 37.47637912631035, AUC: 0.8801180648138652\n",
      "epoch: 32, loss: 37.42245066165924, AUC: 0.8826081940313814\n",
      "epoch: 33, loss: 37.212855100631714, AUC: 0.8852361296277305\n",
      "epoch: 34, loss: 36.963469445705414, AUC: 0.8858770895292789\n",
      "epoch: 35, loss: 37.38724321126938, AUC: 0.8870372269510819\n",
      "epoch: 36, loss: 37.0263529419899, AUC: 0.8892741770074863\n",
      "epoch: 37, loss: 36.71148216724396, AUC: 0.8875756332683827\n",
      "epoch: 38, loss: 36.34175246953964, AUC: 0.8911265511229618\n",
      "epoch: 39, loss: 36.298276364803314, AUC: 0.8920270997846376\n",
      "epoch: 40, loss: 36.19833919405937, AUC: 0.8941871346528562\n",
      "epoch: 41, loss: 36.04595223069191, AUC: 0.8970490206132704\n",
      "epoch: 42, loss: 35.82679411768913, AUC: 0.8970938878063789\n",
      "epoch: 43, loss: 35.568484008312225, AUC: 0.9017216182955594\n",
      "epoch: 44, loss: 35.62436357140541, AUC: 0.8995840170238951\n",
      "epoch: 45, loss: 35.43653428554535, AUC: 0.8998051481899292\n",
      "epoch: 46, loss: 35.37337452173233, AUC: 0.9028400933237617\n",
      "epoch: 47, loss: 35.357267796993256, AUC: 0.9057372320787611\n",
      "epoch: 48, loss: 35.35383886098862, AUC: 0.9042886627012614\n",
      "epoch: 49, loss: 35.04219052195549, AUC: 0.9049071890062558\n",
      "epoch: 50, loss: 34.86956152319908, AUC: 0.9043271202953542\n",
      "epoch: 51, loss: 34.799769788980484, AUC: 0.9090125371756742\n",
      "epoch: 52, loss: 34.65892606973648, AUC: 0.9086632140293304\n",
      "epoch: 53, loss: 34.54585072398186, AUC: 0.9084388780637884\n",
      "epoch: 54, loss: 34.49709439277649, AUC: 0.9099996154240592\n",
      "epoch: 55, loss: 34.641913056373596, AUC: 0.909550943492975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "scores = []\n",
    "for epoch in range(1000):\n",
    "    loss_total = 0\n",
    "    for data in train_loader:\n",
    "        d, p, true_label = data\n",
    "        d, p, true_label  = d.to(device), p.to(device), true_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(d, p)\n",
    "        predicted_label = nn.Softmax(dim=1)(output)\n",
    "        loss = F.cross_entropy( predicted_label, true_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.to('cpu').data.numpy()\n",
    "    pred_y, true_y = [], []\n",
    "    for data in val_loader:\n",
    "        d, p, true_label = data\n",
    "        d, p, true_label  = d.to(device), p.to(device), true_label.to(device)\n",
    "        output = model(d, p)\n",
    "        predicted_label = nn.Softmax(dim=1)(output)[:,1].to('cpu').data.numpy().tolist()\n",
    "        pred_y += predicted_label\n",
    "        true_y += true_label.to('cpu').data.numpy().tolist()\n",
    "    score = roc_auc_score(true_y, pred_y)\n",
    "    if epoch > 10:\n",
    "        if np.mean(scores[-10:]) > score:\n",
    "            scores.append(score)\n",
    "            break\n",
    "    scores.append(score)\n",
    "\n",
    "    print(\"epoch: {}, loss: {}, AUC: {}\".format(epoch, loss_total, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9832330128852834\n"
     ]
    }
   ],
   "source": [
    "pred_y, true_y = [], []\n",
    "for data in train_loader:\n",
    "    d, p, true_label = data\n",
    "    d, p, true_label  = d.to(device), p.to(device), true_label.to(device)\n",
    "    output = model(d, p)\n",
    "    predicted_label = nn.Softmax(dim=1)(output)[:,1].to('cpu').data.numpy().tolist()\n",
    "    pred_y += predicted_label\n",
    "    true_y += true_label.to('cpu').data.numpy().tolist()\n",
    "print(roc_auc_score(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075799917957132\n"
     ]
    }
   ],
   "source": [
    "pred_y, true_y = [], []\n",
    "for data in val_loader:\n",
    "    d, p, true_label = data\n",
    "    d, p, true_label  = d.to(device), p.to(device), true_label.to(device)\n",
    "\n",
    "    output = model(d, p)\n",
    "    predicted_label = nn.Softmax(dim=1)(output)[:,1].to('cpu').data.numpy().tolist()\n",
    "    pred_y += predicted_label\n",
    "    true_y += true_label.to('cpu').data.numpy().tolist()\n",
    "print(roc_auc_score(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201995012468828\n"
     ]
    }
   ],
   "source": [
    "pred_y, true_y = [], []\n",
    "for data in test_loader:\n",
    "    d, p, true_label = data\n",
    "    d, p, true_label  = d.to(device), p.to(device), true_label.to(device)\n",
    "    output = model(d, p)\n",
    "    predicted_label = nn.Softmax(dim=1)(output)[:,1].to('cpu').data.numpy().tolist()\n",
    "    pred_y += predicted_label\n",
    "    true_y += true_label.to('cpu').data.numpy().tolist()\n",
    "print(roc_auc_score(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a2ce01946fc569c1a09c9b4b77c6cb39ea00399395c0b1868d5ff55caf205"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
